<!DOCTYPE HTML>
<!--
	Photon by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>CS 221M</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!-- <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"> -->
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
		<section id="header">
			<div class="inner">
				<h1>
					<span><img src="images/stanford_logo.png" width=80px style="vertical-align: middle;"></span> 
					CS 221M: Mechanistic Interpretability
				</h1>
				<p>Stanford University, Spring 2026</p>
				<ul class="actions special">
					<li><a href="#one" class="button scrolly">Course details</a></li>
				</ul>
			</div>
		</section>

		<section id="one" class="main style1">
			<div class="container">
				<header class="major">
					<h2>Course description</h2>
				</header>
				<p>How do neural networks work internally? This course provides an introduction to *interpretability*, the subfield of machine learning concerned with understanding precisely how models process information and why they produce specific outputs. We will cover topics such as probing, steering, causal abstraction, and sparse autoencoders, with a particular emphasis on causal methods and large language models. The course will include guest lectures from leading interpretability labs across academia and industry.</p>
				<h3>Course Staff</h3>
				<div class="team-container">
					<div class="team-member">
						<div class="profile-layout">
						<img src="images/thomas_icard.png" alt="Thomas Icard" class="headshot">
						<div>
							<span class="member-name">Thomas Icard</span>
							<span class="member-role">Supervisor</span>
						</div>
						</div>
					</div>
					<div class="team-member">
						<div class="profile-layout">
						<img src="images/atticus_geiger.jpg" alt="Atticus Geiger" class="headshot">
						<div>
							<span class="member-name">Atticus Geiger</span>
							<span class="member-role">Lead Instructor</span>
						</div>
						</div>
					</div>
					<div class="team-member">
						<div class="profile-layout">
						<img src="images/amir_zur.jpg" alt="Amir Zur" class="headshot">
						<div>
							<span class="member-name">Amir Zur</span>
							<span class="member-role">Instructor</span>
						</div>
						</div>
					</div>
					<div class="team-member">
						<div class="profile-layout">
						<img src="images/jing_huang.jpg" alt="Jing Huang" class="headshot">
						<div>
							<span class="member-name">Jing Huang</span>
							<span class="member-role">Instructor</span>
						</div>
						</div>
					</div>
					<div class="team-member">
						<div class="profile-layout">
						<img src="images/siri_vatsavaya.jpg" alt="Siri Vatsavaya" class="headshot">
						<div>
							<span class="member-name">Siri Vatsavaya</span>
							<span class="member-role">Instructor</span>
						</div>
						</div>
					</div>
				</div>
				<p></p>
				<h3>Course logistics</h3>
				<ul>
					<li><b>Time:</b> Monday, Wednesday 3:30pm-4:50pm</li>
					<li><b>Location:</b> Monday, Wednesday 3:30pm-4:50pm</li>
					<li><b>Office hours:</b> Monday, Wednesday 3:30pm-4:50pm</li>
				</ul>
			</div>
		</section>
		<hr class="divider-small">
		<!-- One -->
		<section id="two" class="main style1 special">
			<div class="container">
				<!-- <div class="row gtr-150"> -->
					<header class="major">
						<h2>Syllabus</h2>
					</header>

					<table class="table table-bordered">
					<tr style="color: white; background: #59B3A9;">
						<td><b><span style="color: white;">Week</span></b></td>
						<td><b><span style="color: white;">Lesson Topic</span></b></td>
						<td><b><span style="color: white;">Description</span></b></td>
						<td><b><span style="color: white;">Readings</span></b></td>
						<td><b><span style="color: white;">Notes</span></b></td>
					</tr>
					<!-- Week 1 -->
					<tr>
						<td rowspan="2" class="merged">1<br>Intro + Behavioral Analysis</td>
						<td>Intro - course overview, glossary/framing, preview of cool stuff</td>
						<td>Go over course topics/goals. Review neural networks (weights vs. activations) and transformer components (MLPs, attention heads, residual streams). Introduce key concepts of causal analysis (intervention, counterfactual behavior)</td>
						<td>
							<ul>
								<li><a href="https://nlp.seas.harvard.edu/annotated-transformer/">Rush et al. 2018 (annotated transformer)</a></li>
								<li><a href="https://transformer-circuits.pub/2021/framework/index.html">Elhage et al. 2021</a></li>
							</ul>
						</td>
						<td>Transformers Notebook<br><a href="https://colab.research.google.com/gist/AmirZur/cf8a2edcaca3442d45b9640cfa970ce2/lecture-1-introduction-to-language-models.ipynb">Lecture 1: Introduction to Language Models</a></td>
					</tr>
					<tr>
						<td>From the outside: behavioral analysis</td>
						<td>How can we understand a model by intervening on its inputs? Behavioral evaluation with minimal pairs and perturbations. Attributing model behavior to its inputs.</td>
						<td>LIME, SHAP, Integrated gradients, ICL</td>
						<td>Transformers Notebook</td>
					</tr>
					<!-- Week 2 -->
					<tr>
						<td rowspan="2" class="merged">2<br>Probing + Intervention</td>
						<td>Fundamentals + probing and steering</td>
						<td>First step into unpacking the black box. What information can we decode from the activations of a neural network? Static probing (logit lens), trained supervised probes, and difference-in-means probing.</td>
						<td>
							<ul>
								<li>Wendler et al. 2024 (logit lens)</li>
								<li>Hewitt et al. 2019 (supervised probing)</li>
								<li>Marks et al. 2023 (contrastive probing)</li>
							</ul>
						</td>
						<td>NNsight Notebook<br><a href="https://nnsight.net/tutorials/the_geometry_of_truth/">The Geometry of Truth — nnsight</a></td>
					</tr>
					<tr>
						<td>Ablations and Interchanges</td>
						<td>What information is actually used by the model? Overview of different ablation strategies which intervene on the model's activations. Types of ablations: random noise, zero-out, attention head knockouts, null-space. Introduce difference-in-means steering. Conclude with representation fine-tuning (ReFT).</td>
						<td>
							<ul>
								<li>Meng et al. 2022 (ROME)</li>
								<li>Ravfogel et al. (nullspace projection)</li>
								<li>Geva et al. 2023 (knockouts)</li>
								<li>Wu et al. 2024 (ReFT)</li>
							</ul>
						</td>
						<td>NNsight Notebook<br><a href="https://nnsight.net/tutorials/causal_mediation_analysis/">Causal Mediation Analysis I — nnsight</a></td>
					</tr>
					<!-- Week 3 -->
					<tr>
						<td rowspan="2" class="merged">3<br>Causal Mediation</td>
						<td>Causal Mediation (Causal Tracing)</td>
						<td>The basics of causal mediation analysis. How total effect decomposes into indirect and direct effects. An example with causal tracing, where the intervention injects noise and the indirect effect of hidden vectors is measured.</td>
						<td>
							<ul>
								<li>Vig et al. 2020 (Mediation and gender bias)</li>
								<li>Meng et al. 2022 (ROME)</li>
								<li>Mueller et al. 2025 (Quest for the right mediator)</li>
								<li>Conmy et al. 2023 (ACDC)</li>
							</ul>
						</td>
						<td>NNsight Notebook</td>
					</tr>
					<tr>
						<td>Theory of causal abstraction</td>
						<td>The theoretical foundations of "reverse-engineering" neural networks into an algorithm. How neural networks and algorithms are represented by causal models. How abstraction-under-translation underpins computational implementation.</td>
						<td>
							<ul>
								<li>Rubenstein et al. 2017</li>
								<li>Beckers et al. 2019</li>
								<li>Geiger et al. 2021, 2025a,b</li>
							</ul>
						</td>
						<td>Causalab Notebook</td>
					</tr>
					<!-- Week 4 -->
					<tr>
						<td rowspan="2" class="merged">4<br>Causal Abstraction</td>
						<td>Designing Counterfactuals</td>
						<td>How do we localize a causal variable into the internals of a neural network in practice? A framework for hypothesis testing and practical advice on designing counterfactuals.</td>
						<td>
							<ul>
								<li>Wang et al. 2023 (Interp in the wild)</li>
								<li>Prakash et al. 2025 (lookback)</li>
								<li>Gur-Arieh et al. 2025 (mixing mechanisms)</li>
							</ul>
						</td>
						<td>Causalab Notebook</td>
					</tr>
					<tr>
						<td>Desiderata-Based Masking, Distributed Alignment Search</td>
						<td>How can we use machine learning tools to help us localize causal variables? Training binary masks using desiderata articulated in causal terms. Example applications to selecting neurons in MLPs or attention heads.</td>
						<td>
							<ul>
								<li>Davies et al. 2023</li>
								<li>Cao et al. 2020, 2022</li>
								<li>Wu et al. 2023 (Boundless DAS)</li>
								<li>Geiger et al. 2024 (DAS)</li>
							</ul>
						</td>
						<td>Causalab Notebook</td>
					</tr>
					<!-- Week 5 -->
					<tr>
						<td rowspan="2" class="merged">5<br>Feature Geometry</td>
						<td>Variants of the LRH</td>
						<td>What is the geometry of representations in model activations? Overview of the Linear Representation Hypothesis (LRH), evidence for it, and its implications.</td>
						<td>
							<ul>
								<li>Park et al. 2024 (Victor Veitch paper)</li>
								<li>Smolensky 1986</li>
								<li><a href="https://transformer-circuits.pub/2023/linear-representations/index.html">Olah and Jermyn note</a></li>
							</ul>
						</td>
						<td></td>
					</tr>
					<tr>
						<td>Ideas outside the LRH</td>
						<td>When does the Linear Representation Hypothesis fail to describe feature geometry? Feature manifolds, helices, approaches beyond activation space?</td>
						<td>
							<ul>
								<li>Modell et al. 2025 (representation manifolds)</li>
								<li>Kantamneni et al. 2025 (trigonometry for addition)</li>
								<li>Gurnee et al. 2025 (linebreaks)</li>
							</ul>
						</td>
						<td></td>
					</tr>
					<!-- Week 6 -->
					<tr>
						<td rowspan="2" class="merged">6<br>Scalable Unsupervised Decomposition</td>
						<td>Sparse Autoencoders</td>
						<td>How can we decompose model activations at a given location into interpretable features? Polysemanticity, feature superposition, and sparse autoencoders.</td>
						<td>
							<ul>
								<li><a href="https://transformer-circuits.pub/2022/toy_model/index.html">Elhage et al. 2022 (Toy models of superposition)</a></li>
								<li>Cunningham et al. 2023 (SAEs)</li>
							</ul>
						</td>
						<td></td>
					</tr>
					<tr>
						<td>Variants and other stuff</td>
						<td>How do SAEs scale to production LLMs? What assumptions do SAEs bake in, and how can other methods extend them? Cross-layer transcoders, temporal variation, parameter decomposition.</td>
						<td>
							<ul>
								<li>Ameisen et al. 2025 (Circuit tracing)</li>
								<li>Singh et al. 2025 (Priors in time)</li>
								<li>Bushnaq et al. 2025 (SPD)</li>
							</ul>
						</td>
						<td></td>
					</tr>
					<!-- Week 7 -->
					<tr>
						<td rowspan="2" class="merged">7<br>Computational Motifs</td>
						<td>Induction heads, lookback, summarization, copy suppressor</td>
						<td>What "algorithmic primitives" do transformers tend to use? We walk through a few papers which find computational motifs that recur across many studied mechanisms.</td>
						<td></td>
						<td></td>
					</tr>
					<tr>
						<td>–</td>
						<td></td>
						<td></td>
						<td></td>
					</tr>
					<!-- Week 8 -->
					<tr>
						<td rowspan="2" class="merged">8<br>The Role of Training Data</td>
						<td>Memorization, the role of data</td>
						<td>What is memorization: reconstruction vs. counterfactual. How LLMs encode memorized training instances. How training data shapes internal representations.</td>
						<td>
							<ul>
								<li>Zhang et al. 2023 (counterfactual definition)</li>
								<li>Carlini et al. 2021, 2023</li>
								<li>Huang et al. 2024 (counterfactual, localization)</li>
							</ul>
						</td>
						<td></td>
					</tr>
					<tr>
						<td>Developmental interp and training dynamics</td>
						<td>How structures emerge in pre-training of LLMs (interventions on training runs, phase transition).</td>
						<td>Chen et al. 2023</td>
						<td></td>
					</tr>
					<!-- Week 9 -->
					<tr>
						<td rowspan="2" class="merged">9<br>Buffer Day</td>
						<td>Open problems and new directions, biomodels</td>
						<td></td>
						<td></td>
						<td>Elana Simon</td>
					</tr>
					<tr>
						<td>Buffer day</td>
						<td></td>
						<td></td>
						<td></td>
					</tr>
				</table>

					<!-- <div class="col-6 col-12-medium">
						<header class="major">
							<h2>Lorem ipsum dolor adipiscing<br />
							amet dolor consequat</h2>
						</header>
						<p>Adipiscing a commodo ante nunc accumsan et interdum mi ante adipiscing. A nunc lobortis non nisl amet vis sed volutpat aclacus nascetur ac non. Lorem curae et ante amet sapien sed tempus adipiscing id accumsan.</p>
					</div>
					<div class="col-6 col-12-medium imp-medium">
						<span class="image fit"><img src="images/pic01.jpg" alt="" /></span>
					</div> -->
				<!-- </div> -->
			</div>
		</section>

		<!-- Footer -->
		<section id="footer">
			<ul class="actions special">
				<li><a href="https://docs.google.com/forms/d/e/1FAIpQLSel0YSCNeMd3xc1CQ6q1hnvMp-sY4th3OEhcv_gqV2WHdiGPw/viewform" class="button wide primary">Apply here</a></li>
			</ul>
			<!-- <ul class="icons">
				<li><a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a></li>
				<li><a href="#" class="icon brands alt fa-facebook-f"><span class="label">Facebook</span></a></li>
				<li><a href="#" class="icon brands alt fa-instagram"><span class="label">Instagram</span></a></li>
				<li><a href="#" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
				<li><a href="#" class="icon solid alt fa-envelope"><span class="label">Email</span></a></li>
			</ul>
			<ul class="copyright">
				<li>&copy; Untitled</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul> -->
		</section>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>